{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集划分完成\n",
      "训练集大小: 998\n",
      "测试集大小: 250\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "# 1. 加载数据\n",
    "dataX = np.loadtxt('dataset1/dataX.txt')\n",
    "dataY = np.loadtxt('dataset1/dataY.txt')\n",
    "dataZ = np.loadtxt('dataset1/dataZ.txt')\n",
    "dataLabels = np.loadtxt('dataset1/dataLabel.txt')  # 假设每一行有两列\n",
    "\n",
    "# 2. 数据预处理：合并 x, y, z 方向的数据\n",
    "data = np.column_stack((dataX, dataY, dataZ))\n",
    "\n",
    "# 将标签分为两个部分：defect 标签和 severity 标签\n",
    "labels_defect = dataLabels[:, 0].astype(int)  # 道路缺陷标签\n",
    "labels_severity = dataLabels[:, 1].astype(int)  # 严重程度标签\n",
    "\n",
    "# 3. 时域特征提取函数\n",
    "def extract_features(data):\n",
    "    features = []\n",
    "    for group in data:\n",
    "        feature = []\n",
    "        feature.append(np.mean(group))         # 均值\n",
    "        feature.append(np.std(group))          # 标准差\n",
    "        feature.append(np.min(group))          # 最小值\n",
    "        feature.append(np.max(group))          # 最大值\n",
    "        feature.append(np.var(group))          # 方差\n",
    "        feature.append(np.sqrt(np.mean(group**2)))  # 均方根\n",
    "        feature.append(kurtosis(group))        # 峰度\n",
    "        feature.append(skew(group))            # 偏度\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# 提取 x, y, z 三个方向的时域特征\n",
    "features_X = extract_features(dataX)\n",
    "features_Y = extract_features(dataY)\n",
    "features_Z = extract_features(dataZ)\n",
    "\n",
    "# 将特征合并\n",
    "data_features = np.column_stack((features_X, features_Y, features_Z))\n",
    "\n",
    "# 4. 划分数据集\n",
    "X_train, X_test, y_train_defect, y_test_defect, y_train_severity, y_test_severity = train_test_split(\n",
    "    data_features, labels_defect, labels_severity, test_size=0.2, stratify=labels_defect, random_state=42\n",
    ")\n",
    "\n",
    "# 5. 创建 PyTorch 数据加载器\n",
    "# 转换为 PyTorch 张量\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_labels_defect_tensor = torch.tensor(y_train_defect, dtype=torch.long)\n",
    "test_labels_defect_tensor = torch.tensor(y_test_defect, dtype=torch.long)\n",
    "\n",
    "train_labels_severity_tensor = torch.tensor(y_train_severity, dtype=torch.long)\n",
    "test_labels_severity_tensor = torch.tensor(y_test_severity, dtype=torch.long)\n",
    "\n",
    "# 创建 TensorDataset 和 DataLoader\n",
    "train_dataset = TensorDataset(train_tensor, train_labels_defect_tensor, train_labels_severity_tensor)\n",
    "test_dataset = TensorDataset(test_tensor, test_labels_defect_tensor, test_labels_severity_tensor)\n",
    "batch_size = 1024  # 你可以在这里修改批大小\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"数据集划分完成\")\n",
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"测试集大小: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数总数: 11975\n",
      "Input shape: torch.Size([998, 24])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x998 and 64x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 132\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# 6. 训练并测试模型\u001b[39;00m\n\u001b[0;32m    131\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m--> 132\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m test_model(model, test_loader)\n",
      "Cell \u001b[1;32mIn[31], line 65\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m defect_preds, severity_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[0;32m     68\u001b[0m loss_defect \u001b[38;5;241m=\u001b[39m criterion(defect_preds, defect_labels)\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 39\u001b[0m, in \u001b[0;36mMLP_Model_With_Attention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(x)  \u001b[38;5;66;03m# 应用注意力机制\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m defect_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 道路缺陷分类\u001b[39;00m\n\u001b[0;32m     40\u001b[0m severity_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(x)  \u001b[38;5;66;03m# 严重程度分类\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m defect_output, severity_output\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x998 and 64x3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "import time\n",
    "\n",
    "# 1. 定义注意力层\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        # 定义可学习的注意力权重\n",
    "        self.attention_weights = nn.Parameter(torch.randn(input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 计算注意力得分\n",
    "        attention_scores = torch.matmul(x, self.attention_weights)  # [batch_size, input_dim] * [input_dim] -> [batch_size]\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 对输入进行加权\n",
    "        weighted_output = torch.mul(x, attention_scores.unsqueeze(-1))  # 加权输入\n",
    "        return weighted_output.sum(dim=1)  # 对特征进行加权求和\n",
    "\n",
    "# 2. 改进模型以处理时域特征并添加注意力机制\n",
    "class MLP_Model_With_Attention(nn.Module):\n",
    "    def __init__(self, input_size=24):  # 输入维度根据时域特征的数量调整\n",
    "        super(MLP_Model_With_Attention, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.attention = Attention(64)  # 添加注意力机制\n",
    "        self.fc3 = nn.Linear(64, 3)  # 道路缺陷分类输出\n",
    "        self.fc4 = nn.Linear(64, 4)  # 严重程度分类输出\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.attention(x)  # 应用注意力机制\n",
    "        defect_output = self.fc3(x)  # 道路缺陷分类\n",
    "        severity_output = self.fc4(x)  # 严重程度分类\n",
    "        return defect_output, severity_output\n",
    "\n",
    "# 3. 定义损失函数和优化器\n",
    "model = MLP_Model_With_Attention(input_size=24)  # 这里的 input_size 是 8 * 3，每个方向提取8个特征\n",
    "criterion = nn.CrossEntropyLoss()  # 分类任务的损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 计算模型参数数量\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"模型参数总数: {total_params}\")\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # 将模型设置为训练模式\n",
    "    start_time = time.time()  # 记录训练开始时间\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, defect_labels, severity_labels in train_loader:\n",
    "            # 打印输入的形状以检查\n",
    "            print(f'Input shape: {inputs.shape}')\n",
    "            \n",
    "            optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "            # 前向传播\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss_defect = criterion(defect_preds, defect_labels)\n",
    "            loss_severity = criterion(severity_preds, severity_labels)\n",
    "            loss = loss_defect + loss_severity\n",
    "\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # 打印每个 epoch 的平均损失\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    end_time = time.time()  # 记录训练结束时间\n",
    "    print(f\"训练时间: {end_time - start_time} 秒\")\n",
    "\n",
    "# 5. 测试模型并计算精确度、F1值\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # 设置为评估模式\n",
    "    correct_defect = 0\n",
    "    correct_severity = 0\n",
    "    total = 0\n",
    "    all_defect_labels = []\n",
    "    all_severity_labels = []\n",
    "    all_defect_preds = []\n",
    "    all_severity_preds = []\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        for inputs, defect_labels, severity_labels in test_loader:\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "\n",
    "            # 预测\n",
    "            _, predicted_defects = torch.max(defect_preds, 1)\n",
    "            _, predicted_severities = torch.max(severity_preds, 1)\n",
    "\n",
    "            total += defect_labels.size(0)\n",
    "            correct_defect += (predicted_defects == defect_labels).sum().item()\n",
    "            correct_severity += (predicted_severities == severity_labels).sum().item()\n",
    "\n",
    "            # 收集所有预测和真实值，用于计算精确度和F1值\n",
    "            all_defect_labels.extend(defect_labels.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "            all_defect_preds.extend(predicted_defects.cpu().numpy())\n",
    "            all_severity_preds.extend(predicted_severities.cpu().numpy())\n",
    "\n",
    "    # 计算分类准确率\n",
    "    accuracy_defect = 100 * correct_defect / total\n",
    "    accuracy_severity = 100 * correct_severity / total\n",
    "\n",
    "    # 计算精确度和F1值\n",
    "    precision_defect = precision_score(all_defect_labels, all_defect_preds, average='weighted', zero_division=0)\n",
    "    precision_severity = precision_score(all_severity_labels, all_severity_preds, average='weighted', zero_division=0)\n",
    "    f1_defect = f1_score(all_defect_labels, all_defect_preds, average='weighted', zero_division=0)\n",
    "    f1_severity = f1_score(all_severity_labels, all_severity_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f'道路缺陷分类准确率: {accuracy_defect}%')\n",
    "    print(f'严重程度分类准确率: {accuracy_severity}%')\n",
    "    print(f'道路缺陷分类精确度: {precision_defect}')\n",
    "    print(f'严重程度分类精确度: {precision_severity}')\n",
    "    print(f'道路缺陷分类F1值: {f1_defect}')\n",
    "    print(f'严重程度分类F1值: {f1_severity}')\n",
    "\n",
    "# 6. 训练并测试模型\n",
    "num_epochs = 100\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-09-10 21:53:40,759]\u001b[0m Finished trial#0 resulted in value: -0.6599999999999999. Current best value is -0.6599999999999999 with parameters: {'lr': 3.311431091337471e-05, 'batch_size': 64, 'dropout_rate': 0.4502057046997324, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:42,100]\u001b[0m Finished trial#1 resulted in value: -0.786. Current best value is -0.786 with parameters: {'lr': 0.0073762046111866915, 'batch_size': 16, 'dropout_rate': 0.11801314942267851, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:42,475]\u001b[0m Finished trial#2 resulted in value: -0.8300000000000001. Current best value is -0.8300000000000001 with parameters: {'lr': 0.003151156696476796, 'batch_size': 128, 'dropout_rate': 0.3764513161927071, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:42,984]\u001b[0m Finished trial#3 resulted in value: -0.774. Current best value is -0.8300000000000001 with parameters: {'lr': 0.003151156696476796, 'batch_size': 128, 'dropout_rate': 0.3764513161927071, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:44,012]\u001b[0m Finished trial#4 resulted in value: -0.8460000000000001. Current best value is -0.8460000000000001 with parameters: {'lr': 0.0010295147182802671, 'batch_size': 32, 'dropout_rate': 0.22534352627370335, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:44,674]\u001b[0m Finished trial#5 resulted in value: -0.356. Current best value is -0.8460000000000001 with parameters: {'lr': 0.0010295147182802671, 'batch_size': 32, 'dropout_rate': 0.22534352627370335, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:46,666]\u001b[0m Finished trial#6 resulted in value: -0.8340000000000001. Current best value is -0.8460000000000001 with parameters: {'lr': 0.0010295147182802671, 'batch_size': 32, 'dropout_rate': 0.22534352627370335, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:48,666]\u001b[0m Finished trial#7 resulted in value: -0.75. Current best value is -0.8460000000000001 with parameters: {'lr': 0.0010295147182802671, 'batch_size': 32, 'dropout_rate': 0.22534352627370335, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:49,132]\u001b[0m Finished trial#8 resulted in value: -0.6719999999999999. Current best value is -0.8460000000000001 with parameters: {'lr': 0.0010295147182802671, 'batch_size': 32, 'dropout_rate': 0.22534352627370335, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:50,224]\u001b[0m Finished trial#9 resulted in value: -0.854. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:51,303]\u001b[0m Finished trial#10 resulted in value: -0.8200000000000001. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:52,291]\u001b[0m Finished trial#11 resulted in value: -0.8440000000000001. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:53,270]\u001b[0m Finished trial#12 resulted in value: -0.848. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:54,370]\u001b[0m Finished trial#13 resulted in value: -0.8420000000000001. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:55,353]\u001b[0m Finished trial#14 resulted in value: -0.782. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:56,428]\u001b[0m Finished trial#15 resulted in value: -0.8380000000000001. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:57,399]\u001b[0m Finished trial#16 resulted in value: -0.8440000000000001. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:58,507]\u001b[0m Finished trial#17 resulted in value: -0.8. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:59,514]\u001b[0m Finished trial#18 resulted in value: -0.8400000000000001. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:59,951]\u001b[0m Finished trial#19 resulted in value: -0.732. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:00,932]\u001b[0m Finished trial#20 resulted in value: -0.85. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:01,907]\u001b[0m Finished trial#21 resulted in value: -0.854. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:02,895]\u001b[0m Finished trial#22 resulted in value: -0.8180000000000001. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:03,899]\u001b[0m Finished trial#23 resulted in value: -0.854. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:04,890]\u001b[0m Finished trial#24 resulted in value: -0.8400000000000001. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:05,669]\u001b[0m Finished trial#25 resulted in value: -0.75. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:06,657]\u001b[0m Finished trial#26 resulted in value: -0.8220000000000001. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:07,649]\u001b[0m Finished trial#27 resulted in value: -0.852. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:09,537]\u001b[0m Finished trial#28 resulted in value: -0.754. Current best value is -0.854 with parameters: {'lr': 0.00044236564433899894, 'batch_size': 32, 'dropout_rate': 0.12609409454332177, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:10,164]\u001b[0m Finished trial#29 resulted in value: -0.856. Current best value is -0.856 with parameters: {'lr': 0.001146716886472918, 'batch_size': 64, 'dropout_rate': 0.1317007735114974, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:10,790]\u001b[0m Finished trial#30 resulted in value: -0.8340000000000001. Current best value is -0.856 with parameters: {'lr': 0.001146716886472918, 'batch_size': 64, 'dropout_rate': 0.1317007735114974, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:11,406]\u001b[0m Finished trial#31 resulted in value: -0.8340000000000001. Current best value is -0.856 with parameters: {'lr': 0.001146716886472918, 'batch_size': 64, 'dropout_rate': 0.1317007735114974, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:12,026]\u001b[0m Finished trial#32 resulted in value: -0.85. Current best value is -0.856 with parameters: {'lr': 0.001146716886472918, 'batch_size': 64, 'dropout_rate': 0.1317007735114974, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:12,651]\u001b[0m Finished trial#33 resulted in value: -0.8. Current best value is -0.856 with parameters: {'lr': 0.001146716886472918, 'batch_size': 64, 'dropout_rate': 0.1317007735114974, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:13,153]\u001b[0m Finished trial#34 resulted in value: -0.838. Current best value is -0.856 with parameters: {'lr': 0.001146716886472918, 'batch_size': 64, 'dropout_rate': 0.1317007735114974, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:13,552]\u001b[0m Finished trial#35 resulted in value: -0.86. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:13,977]\u001b[0m Finished trial#36 resulted in value: -0.842. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:14,383]\u001b[0m Finished trial#37 resulted in value: -0.856. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:14,796]\u001b[0m Finished trial#38 resulted in value: -0.8400000000000001. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:15,169]\u001b[0m Finished trial#39 resulted in value: -0.774. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:15,569]\u001b[0m Finished trial#40 resulted in value: -0.846. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:15,999]\u001b[0m Finished trial#41 resulted in value: -0.844. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:16,399]\u001b[0m Finished trial#42 resulted in value: -0.852. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:16,835]\u001b[0m Finished trial#43 resulted in value: -0.8460000000000001. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:18,412]\u001b[0m Finished trial#44 resulted in value: -0.8320000000000001. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:18,830]\u001b[0m Finished trial#45 resulted in value: -0.798. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:19,458]\u001b[0m Finished trial#46 resulted in value: -0.85. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:19,853]\u001b[0m Finished trial#47 resulted in value: -0.8400000000000001. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:20,511]\u001b[0m Finished trial#48 resulted in value: -0.8420000000000001. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:54:21,869]\u001b[0m Finished trial#49 resulted in value: -0.8320000000000001. Current best value is -0.86 with parameters: {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'lr': 0.002335055708194196, 'batch_size': 128, 'dropout_rate': 0.13154845364657292, 'hidden_dim': 64}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.78"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###超参数优化\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "# 1. 加载数据\n",
    "dataX = np.loadtxt('dataset1/dataX.txt')\n",
    "dataY = np.loadtxt('dataset1/dataY.txt')\n",
    "dataZ = np.loadtxt('dataset1/dataZ.txt')\n",
    "dataLabels = np.loadtxt('dataset1/dataLabel.txt')  # 假设每一行有两列\n",
    "\n",
    "# 将标签分为两个部分：defect 标签和 severity 标签\n",
    "labels_defect = dataLabels[:, 0].astype(int)  # 道路缺陷标签\n",
    "labels_severity = dataLabels[:, 1].astype(int)  # 严重程度标签\n",
    "\n",
    "# 2. 时域特征提取函数\n",
    "def extract_time_features(data):\n",
    "    features = []\n",
    "    for group in data:\n",
    "        feature = []\n",
    "        feature.append(np.mean(group))         # 均值\n",
    "        feature.append(np.std(group))          # 标准差\n",
    "        feature.append(np.min(group))          # 最小值\n",
    "        feature.append(np.max(group))          # 最大值\n",
    "        feature.append(np.var(group))          # 方差\n",
    "        feature.append(np.sqrt(np.mean(group**2)))  # 均方根\n",
    "        feature.append(kurtosis(group))        # 峰度\n",
    "        feature.append(skew(group))            # 偏度\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# 3. 频域特征提取函数\n",
    "def extract_frequency_features(data):\n",
    "    features = []\n",
    "    for group in data:\n",
    "        fft_vals = np.fft.fft(group)\n",
    "        amplitude_spectrum = np.abs(fft_vals)\n",
    "        phase_spectrum = np.angle(fft_vals)\n",
    "\n",
    "        # 计算频域特征\n",
    "        feature = []\n",
    "        feature.append(np.mean(amplitude_spectrum))        # 幅度谱均值\n",
    "        feature.append(np.std(amplitude_spectrum))         # 幅度谱标准差\n",
    "        feature.append(np.mean(phase_spectrum))            # 相位谱均值\n",
    "        feature.append(np.std(phase_spectrum))             # 相位谱标准差\n",
    "        feature.append(np.mean(amplitude_spectrum[:len(amplitude_spectrum)//2]))  # 低频部分均值\n",
    "        feature.append(np.mean(amplitude_spectrum[len(amplitude_spectrum)//2:]))  # 高频部分均值\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# 提取 x, y, z 三个方向的时域特征和频域特征\n",
    "time_features_X = extract_time_features(dataX)\n",
    "time_features_Y = extract_time_features(dataY)\n",
    "time_features_Z = extract_time_features(dataZ)\n",
    "\n",
    "freq_features_X = extract_frequency_features(dataX)\n",
    "freq_features_Y = extract_frequency_features(dataY)\n",
    "freq_features_Z = extract_frequency_features(dataZ)\n",
    "\n",
    "# 将时域特征和频域特征合并\n",
    "features_X = np.column_stack((time_features_X, freq_features_X))\n",
    "features_Y = np.column_stack((time_features_Y, freq_features_Y))\n",
    "features_Z = np.column_stack((time_features_Z, freq_features_Z))\n",
    "\n",
    "# 将特征合并\n",
    "data_features = np.column_stack((features_X, features_Y, features_Z))\n",
    "\n",
    "# 4. 划分数据集\n",
    "X_train, X_test, y_train_defect, y_test_defect, y_train_severity, y_test_severity = train_test_split(\n",
    "    data_features, labels_defect, labels_severity, test_size=0.2, stratify=labels_defect, random_state=42\n",
    ")\n",
    "\n",
    "# 5. 定义模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_defect = nn.Linear(hidden_dim, 3)  # 道路缺陷分类输出 (假设是 3 类)\n",
    "        self.fc_severity = nn.Linear(hidden_dim, 4)  # 严重程度分类输出 (假设是 4 类)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        defect_output = self.fc_defect(x)\n",
    "        severity_output = self.fc_severity(x)\n",
    "        return defect_output, severity_output\n",
    "\n",
    "# 6. 定义训练和测试过程\n",
    "def train_and_evaluate_model(learning_rate, batch_size, dropout_rate, hidden_dim):\n",
    "    # 创建数据加载器\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                  torch.tensor(y_train_defect, dtype=torch.long),\n",
    "                                  torch.tensor(y_train_severity, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                torch.tensor(y_test_defect, dtype=torch.long),\n",
    "                                torch.tensor(y_test_severity, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 定义模型\n",
    "    model = SimpleNN(input_dim=X_train.shape[1], hidden_dim=hidden_dim, dropout_rate=dropout_rate)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 20\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, defect_labels, severity_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            loss_defect = criterion(defect_preds, defect_labels)\n",
    "            loss_severity = criterion(severity_preds, severity_labels)\n",
    "            loss = loss_defect + loss_severity\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    all_defect_labels = []\n",
    "    all_defect_preds = []\n",
    "    all_severity_labels = []\n",
    "    all_severity_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, defect_labels, severity_labels in val_loader:\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            _, predicted_defects = torch.max(defect_preds, 1)\n",
    "            _, predicted_severities = torch.max(severity_preds, 1)\n",
    "\n",
    "            all_defect_labels.extend(defect_labels.cpu().numpy())\n",
    "            all_defect_preds.extend(predicted_defects.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "            all_severity_preds.extend(predicted_severities.cpu().numpy())\n",
    "\n",
    "    # 计算验证集上的准确率\n",
    "    accuracy_defect = accuracy_score(all_defect_labels, all_defect_preds)\n",
    "    accuracy_severity = accuracy_score(all_severity_labels, all_severity_preds)\n",
    "\n",
    "    avg_accuracy = (accuracy_defect + accuracy_severity) / 2\n",
    "    return -avg_accuracy\n",
    "\n",
    "# 7. 使用 Optuna 进行超参数优化\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [32, 64, 128])\n",
    "\n",
    "    return train_and_evaluate_model(learning_rate, batch_size, dropout_rate, hidden_dim)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)  # 进行 50 次试验\n",
    "\n",
    "# 输出最佳超参数\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# 使用最佳超参数重新训练模型\n",
    "best_params = study.best_params\n",
    "train_and_evaluate_model(best_params['lr'], best_params['batch_size'], best_params['dropout_rate'], best_params['hidden_dim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-09-10 21:52:21,611]\u001b[0m Finished trial#0 resulted in value: -0.812. Current best value is -0.812 with parameters: {'lr': 0.007172032511676547, 'batch_size': 32, 'dropout_rate': 0.37264165841506547, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:22,030]\u001b[0m Finished trial#1 resulted in value: -0.636. Current best value is -0.812 with parameters: {'lr': 0.007172032511676547, 'batch_size': 32, 'dropout_rate': 0.37264165841506547, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:22,457]\u001b[0m Finished trial#2 resulted in value: -0.724. Current best value is -0.812 with parameters: {'lr': 0.007172032511676547, 'batch_size': 32, 'dropout_rate': 0.37264165841506547, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:24,106]\u001b[0m Finished trial#3 resulted in value: -0.794. Current best value is -0.812 with parameters: {'lr': 0.007172032511676547, 'batch_size': 32, 'dropout_rate': 0.37264165841506547, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:24,528]\u001b[0m Finished trial#4 resulted in value: -0.8. Current best value is -0.812 with parameters: {'lr': 0.007172032511676547, 'batch_size': 32, 'dropout_rate': 0.37264165841506547, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:25,534]\u001b[0m Finished trial#5 resulted in value: -0.63. Current best value is -0.812 with parameters: {'lr': 0.007172032511676547, 'batch_size': 32, 'dropout_rate': 0.37264165841506547, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:26,555]\u001b[0m Finished trial#6 resulted in value: -0.798. Current best value is -0.812 with parameters: {'lr': 0.007172032511676547, 'batch_size': 32, 'dropout_rate': 0.37264165841506547, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:27,387]\u001b[0m Finished trial#7 resulted in value: -0.852. Current best value is -0.852 with parameters: {'lr': 0.005078020064432867, 'batch_size': 32, 'dropout_rate': 0.1422066210910208, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:28,426]\u001b[0m Finished trial#8 resulted in value: -0.728. Current best value is -0.852 with parameters: {'lr': 0.005078020064432867, 'batch_size': 32, 'dropout_rate': 0.1422066210910208, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:29,881]\u001b[0m Finished trial#9 resulted in value: -0.6639999999999999. Current best value is -0.852 with parameters: {'lr': 0.005078020064432867, 'batch_size': 32, 'dropout_rate': 0.1422066210910208, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:30,418]\u001b[0m Finished trial#10 resulted in value: -0.784. Current best value is -0.852 with parameters: {'lr': 0.005078020064432867, 'batch_size': 32, 'dropout_rate': 0.1422066210910208, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:31,588]\u001b[0m Finished trial#11 resulted in value: -0.78. Current best value is -0.852 with parameters: {'lr': 0.005078020064432867, 'batch_size': 32, 'dropout_rate': 0.1422066210910208, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:32,743]\u001b[0m Finished trial#12 resulted in value: -0.85. Current best value is -0.852 with parameters: {'lr': 0.005078020064432867, 'batch_size': 32, 'dropout_rate': 0.1422066210910208, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:33,583]\u001b[0m Finished trial#13 resulted in value: -0.8160000000000001. Current best value is -0.852 with parameters: {'lr': 0.005078020064432867, 'batch_size': 32, 'dropout_rate': 0.1422066210910208, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:34,276]\u001b[0m Finished trial#14 resulted in value: -0.854. Current best value is -0.854 with parameters: {'lr': 0.002197705897248066, 'batch_size': 64, 'dropout_rate': 0.10072897997242321, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:34,963]\u001b[0m Finished trial#15 resulted in value: -0.8400000000000001. Current best value is -0.854 with parameters: {'lr': 0.002197705897248066, 'batch_size': 64, 'dropout_rate': 0.10072897997242321, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:35,472]\u001b[0m Finished trial#16 resulted in value: -0.8. Current best value is -0.854 with parameters: {'lr': 0.002197705897248066, 'batch_size': 64, 'dropout_rate': 0.10072897997242321, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:36,168]\u001b[0m Finished trial#17 resulted in value: -0.8360000000000001. Current best value is -0.854 with parameters: {'lr': 0.002197705897248066, 'batch_size': 64, 'dropout_rate': 0.10072897997242321, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:36,679]\u001b[0m Finished trial#18 resulted in value: -0.848. Current best value is -0.854 with parameters: {'lr': 0.002197705897248066, 'batch_size': 64, 'dropout_rate': 0.10072897997242321, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:38,587]\u001b[0m Finished trial#19 resulted in value: -0.848. Current best value is -0.854 with parameters: {'lr': 0.002197705897248066, 'batch_size': 64, 'dropout_rate': 0.10072897997242321, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:39,105]\u001b[0m Finished trial#20 resulted in value: -0.806. Current best value is -0.854 with parameters: {'lr': 0.002197705897248066, 'batch_size': 64, 'dropout_rate': 0.10072897997242321, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:40,247]\u001b[0m Finished trial#21 resulted in value: -0.8460000000000001. Current best value is -0.854 with parameters: {'lr': 0.002197705897248066, 'batch_size': 64, 'dropout_rate': 0.10072897997242321, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:41,393]\u001b[0m Finished trial#22 resulted in value: -0.8380000000000001. Current best value is -0.854 with parameters: {'lr': 0.002197705897248066, 'batch_size': 64, 'dropout_rate': 0.10072897997242321, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:42,514]\u001b[0m Finished trial#23 resulted in value: -0.856. Current best value is -0.856 with parameters: {'lr': 0.0022726403189643057, 'batch_size': 32, 'dropout_rate': 0.13704972826103096, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:43,659]\u001b[0m Finished trial#24 resulted in value: -0.85. Current best value is -0.856 with parameters: {'lr': 0.0022726403189643057, 'batch_size': 32, 'dropout_rate': 0.13704972826103096, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:44,339]\u001b[0m Finished trial#25 resulted in value: -0.8440000000000001. Current best value is -0.856 with parameters: {'lr': 0.0022726403189643057, 'batch_size': 32, 'dropout_rate': 0.13704972826103096, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:45,151]\u001b[0m Finished trial#26 resulted in value: -0.8400000000000001. Current best value is -0.856 with parameters: {'lr': 0.0022726403189643057, 'batch_size': 32, 'dropout_rate': 0.13704972826103096, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:46,284]\u001b[0m Finished trial#27 resulted in value: -0.864. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:46,955]\u001b[0m Finished trial#28 resulted in value: -0.8420000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:47,380]\u001b[0m Finished trial#29 resulted in value: -0.8460000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:49,269]\u001b[0m Finished trial#30 resulted in value: -0.8500000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:50,354]\u001b[0m Finished trial#31 resulted in value: -0.804. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:51,439]\u001b[0m Finished trial#32 resulted in value: -0.782. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:52,532]\u001b[0m Finished trial#33 resulted in value: -0.8260000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:53,321]\u001b[0m Finished trial#34 resulted in value: -0.848. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:53,748]\u001b[0m Finished trial#35 resulted in value: -0.8360000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:54,864]\u001b[0m Finished trial#36 resulted in value: -0.8240000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:55,649]\u001b[0m Finished trial#37 resulted in value: -0.81. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:56,081]\u001b[0m Finished trial#38 resulted in value: -0.788. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:57,042]\u001b[0m Finished trial#39 resulted in value: -0.8460000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:52:58,871]\u001b[0m Finished trial#40 resulted in value: -0.848. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:00,764]\u001b[0m Finished trial#41 resulted in value: -0.8440000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:02,625]\u001b[0m Finished trial#42 resulted in value: -0.8380000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:04,501]\u001b[0m Finished trial#43 resulted in value: -0.806. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:06,096]\u001b[0m Finished trial#44 resulted in value: -0.8460000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:07,997]\u001b[0m Finished trial#45 resulted in value: -0.85. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:08,788]\u001b[0m Finished trial#46 resulted in value: -0.8480000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:09,854]\u001b[0m Finished trial#47 resulted in value: -0.8200000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:10,499]\u001b[0m Finished trial#48 resulted in value: -0.85. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:53:11,283]\u001b[0m Finished trial#49 resulted in value: -0.8160000000000001. Current best value is -0.864 with parameters: {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'lr': 0.0020892701755892877, 'batch_size': 32, 'dropout_rate': 0.12769421124325098, 'hidden_dim': 128}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.8320000000000001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "# 1. 加载数据\n",
    "dataX = np.loadtxt('dataset1/dataX.txt')\n",
    "dataY = np.loadtxt('dataset1/dataY.txt')\n",
    "dataZ = np.loadtxt('dataset1/dataZ.txt')\n",
    "dataLabels = np.loadtxt('dataset1/dataLabel.txt')  # 假设每一行有两列\n",
    "\n",
    "# 将标签分为两个部分：defect 标签和 severity 标签\n",
    "labels_defect = dataLabels[:, 0].astype(int)  # 道路缺陷标签\n",
    "labels_severity = dataLabels[:, 1].astype(int)  # 严重程度标签\n",
    "\n",
    "# 2. 时域特征提取函数\n",
    "def extract_time_features(data):\n",
    "    features = []\n",
    "    for group in data:\n",
    "        feature = []\n",
    "        feature.append(np.mean(group))         # 均值\n",
    "        feature.append(np.std(group))          # 标准差\n",
    "        feature.append(np.min(group))          # 最小值\n",
    "        feature.append(np.max(group))          # 最大值\n",
    "        feature.append(np.var(group))          # 方差\n",
    "        feature.append(np.sqrt(np.mean(group**2)))  # 均方根\n",
    "        feature.append(kurtosis(group))        # 峰度\n",
    "        feature.append(skew(group))            # 偏度\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# 3. 频域特征提取函数\n",
    "def extract_frequency_features(data):\n",
    "    features = []\n",
    "    for group in data:\n",
    "        fft_vals = np.fft.fft(group)\n",
    "        amplitude_spectrum = np.abs(fft_vals)\n",
    "        phase_spectrum = np.angle(fft_vals)\n",
    "\n",
    "        # 计算频域特征\n",
    "        feature = []\n",
    "        feature.append(np.mean(amplitude_spectrum))        # 幅度谱均值\n",
    "        feature.append(np.std(amplitude_spectrum))         # 幅度谱标准差\n",
    "        feature.append(np.mean(phase_spectrum))            # 相位谱均值\n",
    "        feature.append(np.std(phase_spectrum))             # 相位谱标准差\n",
    "        feature.append(np.mean(amplitude_spectrum[:len(amplitude_spectrum)//2]))  # 低频部分均值\n",
    "        feature.append(np.mean(amplitude_spectrum[len(amplitude_spectrum)//2:]))  # 高频部分均值\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# 提取 x, y, z 三个方向的时域特征和频域特征\n",
    "time_features_X = extract_time_features(dataX)\n",
    "time_features_Y = extract_time_features(dataY)\n",
    "time_features_Z = extract_time_features(dataZ)\n",
    "\n",
    "freq_features_X = extract_frequency_features(dataX)\n",
    "freq_features_Y = extract_frequency_features(dataY)\n",
    "freq_features_Z = extract_frequency_features(dataZ)\n",
    "\n",
    "# 将时域特征和频域特征合并\n",
    "features_X = np.column_stack((time_features_X, freq_features_X))\n",
    "features_Y = np.column_stack((time_features_Y, freq_features_Y))\n",
    "features_Z = np.column_stack((time_features_Z, freq_features_Z))\n",
    "\n",
    "# 将特征合并\n",
    "data_features = np.column_stack((features_X, features_Y, features_Z))\n",
    "\n",
    "# 4. 划分数据集\n",
    "X_train, X_test, y_train_defect, y_test_defect, y_train_severity, y_test_severity = train_test_split(\n",
    "    data_features, labels_defect, labels_severity, test_size=0.2, stratify=labels_defect, random_state=42\n",
    ")\n",
    "\n",
    "# 5. 定义模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_defect = nn.Linear(hidden_dim, 3)  # 道路缺陷分类输出 (假设是 3 类)\n",
    "        self.fc_severity = nn.Linear(hidden_dim, 4)  # 严重程度分类输出 (假设是 4 类)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        defect_output = self.fc_defect(x)\n",
    "        severity_output = self.fc_severity(x)\n",
    "        return defect_output, severity_output\n",
    "\n",
    "# 6. 定义训练和测试过程\n",
    "def train_and_evaluate_model(learning_rate, batch_size, dropout_rate, hidden_dim):\n",
    "    # 创建数据加载器\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                  torch.tensor(y_train_defect, dtype=torch.long),\n",
    "                                  torch.tensor(y_train_severity, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                torch.tensor(y_test_defect, dtype=torch.long),\n",
    "                                torch.tensor(y_test_severity, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 定义模型\n",
    "    model = SimpleNN(input_dim=X_train.shape[1], hidden_dim=hidden_dim, dropout_rate=dropout_rate)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 20\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, defect_labels, severity_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            loss_defect = criterion(defect_preds, defect_labels)\n",
    "            loss_severity = criterion(severity_preds, severity_labels)\n",
    "            loss = loss_defect + loss_severity\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    all_defect_labels = []\n",
    "    all_defect_preds = []\n",
    "    all_severity_labels = []\n",
    "    all_severity_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, defect_labels, severity_labels in val_loader:\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            _, predicted_defects = torch.max(defect_preds, 1)\n",
    "            _, predicted_severities = torch.max(severity_preds, 1)\n",
    "\n",
    "            all_defect_labels.extend(defect_labels.cpu().numpy())\n",
    "            all_defect_preds.extend(predicted_defects.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "            all_severity_preds.extend(predicted_severities.cpu().numpy())\n",
    "\n",
    "    # 计算验证集上的准确率\n",
    "    accuracy_defect = accuracy_score(all_defect_labels, all_defect_preds)\n",
    "    accuracy_severity = accuracy_score(all_severity_labels, all_severity_preds)\n",
    "\n",
    "    avg_accuracy = (accuracy_defect + accuracy_severity) / 2\n",
    "    return -avg_accuracy\n",
    "\n",
    "# 7. 使用 Optuna 进行超参数优化\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [32, 64, 128])\n",
    "\n",
    "    return train_and_evaluate_model(learning_rate, batch_size, dropout_rate, hidden_dim)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)  # 进行 50 次试验\n",
    "\n",
    "# 输出最佳超参数\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# 使用最佳超参数重新训练模型\n",
    "best_params = study.best_params\n",
    "train_and_evaluate_model(best_params['lr'], best_params['batch_size'], best_params['dropout_rate'], best_params['hidden_dim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-09-11 15:38:50,815]\u001b[0m Finished trial#0 resulted in value: -0.8320000000000001. Current best value is -0.8320000000000001 with parameters: {'lr': 6.853180274516727e-05, 'batch_size': 32, 'dropout_rate': 0.35534506644482977, 'hidden_dim': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-11 15:38:56,284]\u001b[0m Finished trial#1 resulted in value: -0.852. Current best value is -0.852 with parameters: {'lr': 0.0006411904408964784, 'batch_size': 64, 'dropout_rate': 0.11866703883146675, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-11 15:39:06,088]\u001b[0m Finished trial#2 resulted in value: -0.788. Current best value is -0.852 with parameters: {'lr': 0.0006411904408964784, 'batch_size': 64, 'dropout_rate': 0.11866703883146675, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-11 15:39:26,764]\u001b[0m Finished trial#3 resulted in value: -0.804. Current best value is -0.852 with parameters: {'lr': 0.0006411904408964784, 'batch_size': 64, 'dropout_rate': 0.11866703883146675, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-11 15:39:42,528]\u001b[0m Finished trial#4 resulted in value: -0.848. Current best value is -0.852 with parameters: {'lr': 0.0006411904408964784, 'batch_size': 64, 'dropout_rate': 0.11866703883146675, 'hidden_dim': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-11 15:39:45,956]\u001b[0m Finished trial#5 resulted in value: -0.8460000000000001. Current best value is -0.852 with parameters: {'lr': 0.0006411904408964784, 'batch_size': 64, 'dropout_rate': 0.11866703883146675, 'hidden_dim': 64}.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 164\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_and_evaluate_model(learning_rate, batch_size, dropout_rate, hidden_dim)\n\u001b[0;32m    163\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 164\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 进行 50 次试验\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# 输出最佳超参数\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study.py:280\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m \n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 280\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_parallel(func, n_trials, timeout, n_jobs, catch)\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study.py:395\u001b[0m, in \u001b[0;36mStudy._optimize_sequential\u001b[1;34m(self, func, n_trials, timeout, catch)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elapsed_seconds \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m timeout:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study.py:469\u001b[0m, in \u001b[0;36mStudy._run_trial\u001b[1;34m(self, func, catch)\u001b[0m\n\u001b[0;32m    466\u001b[0m trial_number \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mnumber\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 469\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m structs\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    471\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSetting status of trial#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trial_number,\n\u001b[0;32m    472\u001b[0m                                                             structs\u001b[38;5;241m.\u001b[39mTrialState\u001b[38;5;241m.\u001b[39mPRUNED,\n\u001b[0;32m    473\u001b[0m                                                             \u001b[38;5;28mstr\u001b[39m(e))\n",
      "Cell \u001b[1;32mIn[26], line 161\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    158\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_uniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    159\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m])\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 123\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(learning_rate, batch_size, dropout_rate, hidden_dim)\u001b[0m\n\u001b[0;32m    121\u001b[0m loss_severity \u001b[38;5;241m=\u001b[39m criterion(severity_preds, severity_labels)\n\u001b[0;32m    122\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_defect \u001b[38;5;241m+\u001b[39m loss_severity\n\u001b[1;32m--> 123\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    125\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZSCAV\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###只使用x，z轴两个通道的数据\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "# 1. 加载数据\n",
    "dataX = np.loadtxt('dataset1/dataX.txt')\n",
    "dataZ = np.loadtxt('dataset1/dataZ.txt')\n",
    "dataLabels = np.loadtxt('dataset1/dataLabel.txt')  # 假设每一行有两列\n",
    "\n",
    "# 将标签分为两个部分：defect 标签和 severity 标签\n",
    "labels_defect = dataLabels[:, 0].astype(int)  # 道路缺陷标签\n",
    "labels_severity = dataLabels[:, 1].astype(int)  # 严重程度标签\n",
    "\n",
    "# 2. 时域特征提取函数\n",
    "def extract_time_features(data):\n",
    "    features = []\n",
    "    for group in data:\n",
    "        feature = []\n",
    "        feature.append(np.mean(group))         # 均值\n",
    "        feature.append(np.std(group))          # 标准差\n",
    "        feature.append(np.min(group))          # 最小值\n",
    "        feature.append(np.max(group))          # 最大值\n",
    "        feature.append(np.var(group))          # 方差\n",
    "        feature.append(np.sqrt(np.mean(group**2)))  # 均方根\n",
    "        feature.append(kurtosis(group))        # 峰度\n",
    "        feature.append(skew(group))            # 偏度\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# 3. 频域特征提取函数\n",
    "def extract_frequency_features(data):\n",
    "    features = []\n",
    "    for group in data:\n",
    "        fft_vals = np.fft.fft(group)\n",
    "        amplitude_spectrum = np.abs(fft_vals)\n",
    "        phase_spectrum = np.angle(fft_vals)\n",
    "\n",
    "        # 计算频域特征\n",
    "        feature = []\n",
    "        feature.append(np.mean(amplitude_spectrum))        # 幅度谱均值\n",
    "        feature.append(np.std(amplitude_spectrum))         # 幅度谱标准差\n",
    "        feature.append(np.mean(phase_spectrum))            # 相位谱均值\n",
    "        feature.append(np.std(phase_spectrum))             # 相位谱标准差\n",
    "        feature.append(np.mean(amplitude_spectrum[:len(amplitude_spectrum)//2]))  # 低频部分均值\n",
    "        feature.append(np.mean(amplitude_spectrum[len(amplitude_spectrum)//2:]))  # 高频部分均值\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# 提取 x 和 z 轴的时域特征和频域特征\n",
    "time_features_X = extract_time_features(dataX)\n",
    "time_features_Z = extract_time_features(dataZ)\n",
    "\n",
    "freq_features_X = extract_frequency_features(dataX)\n",
    "freq_features_Z = extract_frequency_features(dataZ)\n",
    "\n",
    "# 将时域特征和频域特征合并\n",
    "features_X = np.column_stack((time_features_X, freq_features_X))\n",
    "features_Z = np.column_stack((time_features_Z, freq_features_Z))\n",
    "\n",
    "# 将 x 和 z 轴特征合并\n",
    "data_features = np.column_stack((features_X, features_Z))\n",
    "\n",
    "# 4. 划分数据集\n",
    "X_train, X_test, y_train_defect, y_test_defect, y_train_severity, y_test_severity = train_test_split(\n",
    "    data_features, labels_defect, labels_severity, test_size=0.2, stratify=labels_defect, random_state=42\n",
    ")\n",
    "\n",
    "# 5. 定义模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_defect = nn.Linear(hidden_dim, 3)  # 道路缺陷分类输出 (假设是 3 类)\n",
    "        self.fc_severity = nn.Linear(hidden_dim, 4)  # 严重程度分类输出 (假设是 4 类)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        defect_output = self.fc_defect(x)\n",
    "        severity_output = self.fc_severity(x)\n",
    "        return defect_output, severity_output\n",
    "\n",
    "# 6. 定义训练和测试过程\n",
    "def train_and_evaluate_model(learning_rate, batch_size, dropout_rate, hidden_dim):\n",
    "    # 创建数据加载器\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                  torch.tensor(y_train_defect, dtype=torch.long),\n",
    "                                  torch.tensor(y_train_severity, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                torch.tensor(y_test_defect, dtype=torch.long),\n",
    "                                torch.tensor(y_test_severity, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 定义模型\n",
    "    model = SimpleNN(input_dim=X_train.shape[1], hidden_dim=hidden_dim, dropout_rate=dropout_rate)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 200\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, defect_labels, severity_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            loss_defect = criterion(defect_preds, defect_labels)\n",
    "            loss_severity = criterion(severity_preds, severity_labels)\n",
    "            loss = loss_defect + loss_severity\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    all_defect_labels = []\n",
    "    all_defect_preds = []\n",
    "    all_severity_labels = []\n",
    "    all_severity_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, defect_labels, severity_labels in val_loader:\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            _, predicted_defects = torch.max(defect_preds, 1)\n",
    "            _, predicted_severities = torch.max(severity_preds, 1)\n",
    "\n",
    "            all_defect_labels.extend(defect_labels.cpu().numpy())\n",
    "            all_defect_preds.extend(predicted_defects.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "            all_severity_preds.extend(predicted_severities.cpu().numpy())\n",
    "\n",
    "    # 计算验证集上的准确率\n",
    "    accuracy_defect = accuracy_score(all_defect_labels, all_defect_preds)\n",
    "    accuracy_severity = accuracy_score(all_severity_labels, all_severity_preds)\n",
    "\n",
    "    avg_accuracy = (accuracy_defect + accuracy_severity) / 2\n",
    "    return -avg_accuracy\n",
    "\n",
    "# 7. 使用 Optuna 进行超参数优化\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [32, 64, 128])\n",
    "\n",
    "    return train_and_evaluate_model(learning_rate, batch_size, dropout_rate, hidden_dim)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)  # 进行 50 次试验\n",
    "\n",
    "# 输出最佳超参数\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# 使用最佳超参数重新训练模型\n",
    "best_params = study.best_params\n",
    "train_and_evaluate_model(best_params['lr'], best_params['batch_size'], best_params['dropout_rate'], best_params['hidden_dim'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
