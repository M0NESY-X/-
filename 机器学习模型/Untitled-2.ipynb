{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from scipy.signal import resample\n",
    "\n",
    "# 数据增强 - 添加噪声\n",
    "def add_noise(data, noise_factor=0.01):\n",
    "    noise = noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "# 数据增强 - 时间平移\n",
    "def time_shift(data, shift_max=5):\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    return np.roll(data, shift, axis=0)\n",
    "\n",
    "# 数据增强 - 时间缩放\n",
    "def time_stretch(data, stretch_factor=0.8):\n",
    "    length = int(data.shape[0] * stretch_factor)\n",
    "    data = resample(data, length)\n",
    "    if length != 50:  # 强制重采样为50个点\n",
    "        data = resample(data, 50)\n",
    "    return data\n",
    "\n",
    "# 综合数据增强函数\n",
    "def augment_data(data):\n",
    "    # 确保数据是二维的，每行对应一个时间步，列对应不同的通道\n",
    "    assert data.shape == (50, 3), f\"数据形状不正确，期望 (50, 3)，但得到了 {data.shape}\"\n",
    "    \n",
    "    augmented_data = np.empty_like(data)  # 初始化增强后的数据\n",
    "    for i in range(3):  # 遍历每个通道 (x, y, z)\n",
    "        channel_data = data[:, i]  # 提取单个通道的数据\n",
    "        channel_data = add_noise(channel_data)\n",
    "        channel_data = time_shift(channel_data)\n",
    "        channel_data = time_stretch(channel_data, stretch_factor=0.8)\n",
    "        augmented_data[:, i] = channel_data  # 将增强后的通道数据放回去\n",
    "    return augmented_data\n",
    "\n",
    "# 自定义数据集，包含数据增强\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, data, labels_defect, labels_severity, augment=True):\n",
    "        self.data = data\n",
    "        self.labels_defect = labels_defect\n",
    "        self.labels_severity = labels_severity\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx].reshape(50, 3)  # 将每个样本 reshape 成 (50, 3)\n",
    "        label_defect = self.labels_defect[idx]\n",
    "        label_severity = self.labels_severity[idx]\n",
    "    \n",
    "         # 如果 augment=True，进行数据增强\n",
    "        if self.augment:\n",
    "          sample = augment_data(sample)\n",
    "\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(label_defect, dtype=torch.long), torch.tensor(label_severity, dtype=torch.long)\n",
    "\n",
    "# 1. 加载数据\n",
    "dataX = np.loadtxt('dataset1/dataX.txt')  # 每组数据50个点\n",
    "dataY = np.loadtxt('dataset1/dataY.txt')\n",
    "dataZ = np.loadtxt('dataset1/dataZ.txt')\n",
    "dataLabels = np.loadtxt('dataset1/dataLabel.txt')  # 假设每一行有两列\n",
    "\n",
    "# 将标签分为两个部分：defect 标签和 severity 标签\n",
    "labels_defect = dataLabels[:, 0].astype(int)  # 道路缺陷标签\n",
    "labels_severity = dataLabels[:, 1].astype(int)  # 严重程度标签\n",
    "\n",
    "# 2. 数据预处理：合并 x, y, z 方向的数据\n",
    "data = np.column_stack((dataX, dataY, dataZ))  # [样本数, 150]\n",
    "\n",
    "# 3. 划分数据集\n",
    "X_train, X_test, y_train_defect, y_test_defect, y_train_severity, y_test_severity = train_test_split(\n",
    "    data, labels_defect, labels_severity, test_size=0.2, stratify=labels_defect, random_state=42\n",
    ")\n",
    "\n",
    "# 创建数据加载器时应用增强数据集\n",
    "train_dataset = AugmentedDataset(X_train, y_train_defect, y_train_severity, augment=True)\n",
    "test_dataset = AugmentedDataset(X_test, y_test_defect, y_test_severity, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5120, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5120, shuffle=False)\n",
    "\n",
    "class Simplified_CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simplified_CNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 12, 64)  # Adjusted input size after pooling\n",
    "        self.dropout = nn.Dropout(0.3)  # Reduced dropout to allow more learning\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "\n",
    "        self.fc_defect = nn.Linear(32, 3)  # Defect classification (3 classes)\n",
    "        self.fc_severity = nn.Linear(32, 4)  # Severity classification (4 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 50, 3).permute(0, 2, 1)\n",
    "\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        x = x.view(-1, 32 * 12)  # Adjust size based on the layer output\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        defect_output = self.fc_defect(x)\n",
    "        severity_output = self.fc_severity(x)\n",
    "\n",
    "        return defect_output, severity_output\n",
    "\n",
    "\n",
    "# 创建模型实例\n",
    "model = Simplified_CNN_Model()\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, defect_labels, severity_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            loss_defect = criterion(defect_preds, defect_labels)\n",
    "            loss_severity = criterion(severity_preds, severity_labels)\n",
    "            loss = loss_defect + loss_severity\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# 测试模型\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_defect_labels = []\n",
    "    all_defect_preds = []\n",
    "    all_severity_labels = []\n",
    "    all_severity_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, defect_labels, severity_labels in test_loader:\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            _, predicted_defects = torch.max(defect_preds, 1)\n",
    "            _, predicted_severities = torch.max(severity_preds, 1)\n",
    "\n",
    "            all_defect_labels.extend(defect_labels.cpu().numpy())\n",
    "            all_defect_preds.extend(predicted_defects.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "            all_severity_preds.extend(predicted_severities.cpu().numpy())\n",
    "\n",
    "    accuracy_defect = accuracy_score(all_defect_labels, all_defect_preds)\n",
    "    precision_defect = precision_score(all_defect_labels, all_defect_preds, average='weighted', zero_division=0)\n",
    "    f1_defect = f1_score(all_defect_labels, all_defect_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    accuracy_severity = accuracy_score(all_severity_labels, all_severity_preds)\n",
    "    precision_severity = precision_score(all_severity_labels, all_severity_preds, average='weighted', zero_division=0)\n",
    "    f1_severity = f1_score(all_severity_labels, all_severity_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f'道路缺陷分类准确率: {accuracy_defect * 100:.2f}%')\n",
    "    print(f'严重程度分类准确率: {accuracy_severity * 100:.2f}%')\n",
    "    print(f'道路缺陷分类精确度: {precision_defect:.2f}')\n",
    "    print(f'严重程度分类精确度: {precision_severity:.2f}')\n",
    "    print(f'道路缺陷分类F1值: {f1_defect:.2f}')\n",
    "    print(f'严重程度分类F1值: {f1_severity:.2f}')\n",
    "\n",
    "# 开始训练和测试\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=200)\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-09-10 21:01:20,930]\u001b[0m Finished trial#0 resulted in value: -0.605. Current best value is -0.605 with parameters: {'lr': 0.002298583419284908, 'batch_size': 32, 'dropout_rate': 0.3865475393880122, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:22,482]\u001b[0m Finished trial#1 resulted in value: -0.63. Current best value is -0.63 with parameters: {'lr': 0.0009387583095713397, 'batch_size': 128, 'dropout_rate': 0.25203985525161676, 'num_filters1': 64, 'num_filters2': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:24,937]\u001b[0m Finished trial#2 resulted in value: -0.61. Current best value is -0.63 with parameters: {'lr': 0.0009387583095713397, 'batch_size': 128, 'dropout_rate': 0.25203985525161676, 'num_filters1': 64, 'num_filters2': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:26,478]\u001b[0m Finished trial#3 resulted in value: -0.61. Current best value is -0.63 with parameters: {'lr': 0.0009387583095713397, 'batch_size': 128, 'dropout_rate': 0.25203985525161676, 'num_filters1': 64, 'num_filters2': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:28,904]\u001b[0m Finished trial#4 resulted in value: -0.6775. Current best value is -0.6775 with parameters: {'lr': 0.0001183030753491647, 'batch_size': 32, 'dropout_rate': 0.20546067721349273, 'num_filters1': 16, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:30,468]\u001b[0m Finished trial#5 resulted in value: -0.6875. Current best value is -0.6875 with parameters: {'lr': 0.00016175065074991674, 'batch_size': 128, 'dropout_rate': 0.41142592029491565, 'num_filters1': 64, 'num_filters2': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:32,611]\u001b[0m Finished trial#6 resulted in value: -0.67. Current best value is -0.6875 with parameters: {'lr': 0.00016175065074991674, 'batch_size': 128, 'dropout_rate': 0.41142592029491565, 'num_filters1': 64, 'num_filters2': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:34,149]\u001b[0m Finished trial#7 resulted in value: -0.6125. Current best value is -0.6875 with parameters: {'lr': 0.00016175065074991674, 'batch_size': 128, 'dropout_rate': 0.41142592029491565, 'num_filters1': 64, 'num_filters2': 32}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:35,809]\u001b[0m Finished trial#8 resulted in value: -0.6975. Current best value is -0.6975 with parameters: {'lr': 0.0006755905413599839, 'batch_size': 128, 'dropout_rate': 0.3170469717972436, 'num_filters1': 32, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:37,673]\u001b[0m Finished trial#9 resulted in value: -0.635. Current best value is -0.6975 with parameters: {'lr': 0.0006755905413599839, 'batch_size': 128, 'dropout_rate': 0.3170469717972436, 'num_filters1': 32, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:39,708]\u001b[0m Finished trial#10 resulted in value: -0.6799999999999999. Current best value is -0.6975 with parameters: {'lr': 0.0006755905413599839, 'batch_size': 128, 'dropout_rate': 0.3170469717972436, 'num_filters1': 32, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:41,391]\u001b[0m Finished trial#11 resulted in value: -0.6075. Current best value is -0.6975 with parameters: {'lr': 0.0006755905413599839, 'batch_size': 128, 'dropout_rate': 0.3170469717972436, 'num_filters1': 32, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:43,214]\u001b[0m Finished trial#12 resulted in value: -0.655. Current best value is -0.6975 with parameters: {'lr': 0.0006755905413599839, 'batch_size': 128, 'dropout_rate': 0.3170469717972436, 'num_filters1': 32, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:44,995]\u001b[0m Finished trial#13 resulted in value: -0.6575. Current best value is -0.6975 with parameters: {'lr': 0.0006755905413599839, 'batch_size': 128, 'dropout_rate': 0.3170469717972436, 'num_filters1': 32, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:46,865]\u001b[0m Finished trial#14 resulted in value: -0.69. Current best value is -0.6975 with parameters: {'lr': 0.0006755905413599839, 'batch_size': 128, 'dropout_rate': 0.3170469717972436, 'num_filters1': 32, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:48,794]\u001b[0m Finished trial#15 resulted in value: -0.7. Current best value is -0.7 with parameters: {'lr': 0.0005419731824757451, 'batch_size': 128, 'dropout_rate': 0.4572632981397161, 'num_filters1': 64, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:50,802]\u001b[0m Finished trial#16 resulted in value: -0.59. Current best value is -0.7 with parameters: {'lr': 0.0005419731824757451, 'batch_size': 128, 'dropout_rate': 0.4572632981397161, 'num_filters1': 64, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:52,499]\u001b[0m Finished trial#17 resulted in value: -0.5675. Current best value is -0.7 with parameters: {'lr': 0.0005419731824757451, 'batch_size': 128, 'dropout_rate': 0.4572632981397161, 'num_filters1': 64, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:54,236]\u001b[0m Finished trial#18 resulted in value: -0.675. Current best value is -0.7 with parameters: {'lr': 0.0005419731824757451, 'batch_size': 128, 'dropout_rate': 0.4572632981397161, 'num_filters1': 64, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:01:56,047]\u001b[0m Finished trial#19 resulted in value: -0.6425000000000001. Current best value is -0.7 with parameters: {'lr': 0.0005419731824757451, 'batch_size': 128, 'dropout_rate': 0.4572632981397161, 'num_filters1': 64, 'num_filters2': 128}.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳超参数: \n",
      "{'lr': 0.0005419731824757451, 'batch_size': 128, 'dropout_rate': 0.4572632981397161, 'num_filters1': 64, 'num_filters2': 128}\n",
      "Epoch [1/20], Loss: 2.2496799677610397\n",
      "Epoch [2/20], Loss: 1.788940668106079\n",
      "Epoch [3/20], Loss: 1.6425750255584717\n",
      "Epoch [4/20], Loss: 1.533383309841156\n",
      "Epoch [5/20], Loss: 1.4740909039974213\n",
      "Epoch [6/20], Loss: 1.4282962828874588\n",
      "Epoch [7/20], Loss: 1.3928359746932983\n",
      "Epoch [8/20], Loss: 1.3654481768608093\n",
      "Epoch [9/20], Loss: 1.2847770005464554\n",
      "Epoch [10/20], Loss: 1.303515538573265\n",
      "Epoch [11/20], Loss: 1.301405742764473\n",
      "Epoch [12/20], Loss: 1.2828572392463684\n",
      "Epoch [13/20], Loss: 1.2489109337329865\n",
      "Epoch [14/20], Loss: 1.2355439141392708\n",
      "Epoch [15/20], Loss: 1.2427116930484772\n",
      "Epoch [16/20], Loss: 1.2360970228910446\n",
      "Epoch [17/20], Loss: 1.2608822733163834\n",
      "Epoch [18/20], Loss: 1.2593141347169876\n",
      "Epoch [19/20], Loss: 1.2058682143688202\n",
      "Epoch [20/20], Loss: 1.1775417625904083\n",
      "道路缺陷分类准确率: 57.20%\n",
      "严重程度分类准确率: 48.40%\n",
      "道路缺陷分类精确度: 0.75\n",
      "严重程度分类精确度: 0.69\n",
      "道路缺陷分类F1值: 0.53\n",
      "严重程度分类F1值: 0.44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from scipy.signal import resample\n",
    "import optuna  # 确保已安装 optuna 库\n",
    "\n",
    "# 数据增强函数\n",
    "def add_noise(data, noise_factor=0.01):\n",
    "    noise = noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "def time_shift(data, shift_max=5):\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    return np.roll(data, shift, axis=0)\n",
    "\n",
    "def time_stretch(data, stretch_factor=0.8):\n",
    "    length = int(data.shape[0] * stretch_factor)\n",
    "    data = resample(data, length)\n",
    "    if length != 50:  # 强制重采样为50个点\n",
    "        data = resample(data, 50)\n",
    "    return data\n",
    "\n",
    "def augment_data(data):\n",
    "    # 确保输入数据形状为 (50, 3)\n",
    "    data = data.reshape(50, 3)\n",
    "    augmented_data = np.empty_like(data)\n",
    "    for i in range(3):  # 对每个通道进行增强\n",
    "        channel_data = data[:, i]\n",
    "        channel_data = add_noise(channel_data)\n",
    "        channel_data = time_shift(channel_data)\n",
    "        channel_data = time_stretch(channel_data, stretch_factor=0.8)\n",
    "        augmented_data[:, i] = channel_data\n",
    "    return augmented_data\n",
    "\n",
    "# 自定义数据集，包含数据增强\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, data, labels_defect, labels_severity, augment=True):\n",
    "        self.data = data\n",
    "        self.labels_defect = labels_defect\n",
    "        self.labels_severity = labels_severity\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label_defect = self.labels_defect[idx]\n",
    "        label_severity = self.labels_severity[idx]\n",
    "\n",
    "        # 如果 augment=True，进行数据增强\n",
    "        if self.augment:\n",
    "            sample = augment_data(sample)\n",
    "        else:\n",
    "            sample = sample.reshape(50, 3)  # 保证形状一致\n",
    "\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(label_defect, dtype=torch.long), torch.tensor(label_severity, dtype=torch.long)\n",
    "\n",
    "# 加载数据\n",
    "dataX = np.loadtxt('dataset1/dataX.txt')  # 每组数据50个点\n",
    "dataY = np.loadtxt('dataset1/dataY.txt')\n",
    "dataZ = np.loadtxt('dataset1/dataZ.txt')\n",
    "dataLabels = np.loadtxt('dataset1/dataLabel.txt')  # 假设每一行有两列\n",
    "\n",
    "# 将标签分为两个部分：defect 标签和 severity 标签\n",
    "labels_defect = dataLabels[:, 0].astype(int)  # 道路缺陷标签\n",
    "labels_severity = dataLabels[:, 1].astype(int)  # 严重程度标签\n",
    "\n",
    "# 数据预处理：合并 x, y, z 方向的数据\n",
    "data = np.hstack((dataX, dataY, dataZ))  # [样本数, 150]\n",
    "\n",
    "# 划分数据集\n",
    "X_train_val, X_test, y_train_val_defect, y_test_defect, y_train_val_severity, y_test_severity = train_test_split(\n",
    "    data, labels_defect, labels_severity, test_size=0.2, stratify=labels_defect, random_state=42\n",
    ")\n",
    "\n",
    "def objective(trial):\n",
    "    # 定义需要优化的超参数\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
    "    num_filters1 = trial.suggest_categorical('num_filters1', [16, 32, 64])\n",
    "    num_filters2 = trial.suggest_categorical('num_filters2', [32, 64, 128])\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    X_train, X_val, y_train_defect, y_val_defect, y_train_severity, y_val_severity = train_test_split(\n",
    "        X_train_val, y_train_val_defect, y_train_val_severity, test_size=0.2, stratify=y_train_val_defect, random_state=42\n",
    "    )\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataset = AugmentedDataset(X_train, y_train_defect, y_train_severity, augment=True)\n",
    "    val_dataset = AugmentedDataset(X_val, y_val_defect, y_val_severity, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 定义模型\n",
    "    class CNN_Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN_Model, self).__init__()\n",
    "            self.conv1 = nn.Conv1d(in_channels=3, out_channels=num_filters1, kernel_size=5, stride=1, padding=2)\n",
    "            self.bn1 = nn.BatchNorm1d(num_filters1)\n",
    "            self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.conv2 = nn.Conv1d(in_channels=num_filters1, out_channels=num_filters2, kernel_size=3, stride=1, padding=1)\n",
    "            self.bn2 = nn.BatchNorm1d(num_filters2)\n",
    "            self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.fc1 = nn.Linear(num_filters2 * 12, 64)\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "\n",
    "            self.fc_defect = nn.Linear(32, 3)  # 道路缺陷分类 (3 类)\n",
    "            self.fc_severity = nn.Linear(32, 4)  # 严重程度分类 (4 类)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)  # [batch_size, 3, 50]\n",
    "\n",
    "            x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = torch.relu(self.fc2(x))\n",
    "\n",
    "            defect_output = self.fc_defect(x)\n",
    "            severity_output = self.fc_severity(x)\n",
    "\n",
    "            return defect_output, severity_output\n",
    "\n",
    "    model = CNN_Model()\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 10  # 为了加快优化过程，设置较小的 epoch 数\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, defect_labels, severity_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            loss_defect = criterion(defect_preds, defect_labels)\n",
    "            loss_severity = criterion(severity_preds, severity_labels)\n",
    "            loss = loss_defect + loss_severity\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    all_defect_labels = []\n",
    "    all_defect_preds = []\n",
    "    all_severity_labels = []\n",
    "    all_severity_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, defect_labels, severity_labels in val_loader:\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            _, predicted_defects = torch.max(defect_preds, 1)\n",
    "            _, predicted_severities = torch.max(severity_preds, 1)\n",
    "\n",
    "            all_defect_labels.extend(defect_labels.cpu().numpy())\n",
    "            all_defect_preds.extend(predicted_defects.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "            all_severity_preds.extend(predicted_severities.cpu().numpy())\n",
    "\n",
    "    # 计算验证集上的准确率\n",
    "    accuracy_defect = accuracy_score(all_defect_labels, all_defect_preds)\n",
    "    accuracy_severity = accuracy_score(all_severity_labels, all_severity_preds)\n",
    "\n",
    "    # 返回验证集上平均准确率的负值，Optuna 默认最小化目标\n",
    "    avg_accuracy = (accuracy_defect + accuracy_severity) / 2\n",
    "    return -avg_accuracy\n",
    "\n",
    "# 创建 Optuna study 并开始优化\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# 输出最佳超参数\n",
    "print('最佳超参数: ')\n",
    "print(study.best_params)\n",
    "\n",
    "# 使用最佳超参数重新训练模型并在测试集上评估\n",
    "best_params = study.best_params\n",
    "learning_rate = best_params['lr']\n",
    "batch_size = best_params['batch_size']\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "num_filters1 = best_params['num_filters1']\n",
    "num_filters2 = best_params['num_filters2']\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = AugmentedDataset(X_train_val, y_train_val_defect, y_train_val_severity, augment=True)\n",
    "test_dataset = AugmentedDataset(X_test, y_test_defect, y_test_severity, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义模型\n",
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=num_filters1, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(num_filters1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_filters1, out_channels=num_filters2, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(num_filters2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(num_filters2 * 12, 64)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "\n",
    "        self.fc_defect = nn.Linear(32, 3)  # 道路缺陷分类 (3 类)\n",
    "        self.fc_severity = nn.Linear(32, 4)  # 严重程度分类 (4 类)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, 3, 50]\n",
    "\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        defect_output = self.fc_defect(x)\n",
    "        severity_output = self.fc_severity(x)\n",
    "\n",
    "        return defect_output, severity_output\n",
    "\n",
    "model = CNN_Model()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 20\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, defect_labels, severity_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        defect_preds, severity_preds = model(inputs)\n",
    "        loss_defect = criterion(defect_preds, defect_labels)\n",
    "        loss_severity = criterion(severity_preds, severity_labels)\n",
    "        loss = loss_defect + loss_severity\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# 测试模型\n",
    "model.eval()\n",
    "all_defect_labels = []\n",
    "all_defect_preds = []\n",
    "all_severity_labels = []\n",
    "all_severity_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, defect_labels, severity_labels in test_loader:\n",
    "        defect_preds, severity_preds = model(inputs)\n",
    "        _, predicted_defects = torch.max(defect_preds, 1)\n",
    "        _, predicted_severities = torch.max(severity_preds, 1)\n",
    "\n",
    "        all_defect_labels.extend(defect_labels.cpu().numpy())\n",
    "        all_defect_preds.extend(predicted_defects.cpu().numpy())\n",
    "        all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "        all_severity_preds.extend(predicted_severities.cpu().numpy())\n",
    "\n",
    "# 计算测试集上的指标\n",
    "accuracy_defect = accuracy_score(all_defect_labels, all_defect_preds)\n",
    "precision_defect = precision_score(all_defect_labels, all_defect_preds, average='weighted', zero_division=0)\n",
    "f1_defect = f1_score(all_defect_labels, all_defect_preds, average='weighted', zero_division=0)\n",
    "\n",
    "accuracy_severity = accuracy_score(all_severity_labels, all_severity_preds)\n",
    "precision_severity = precision_score(all_severity_labels, all_severity_preds, average='weighted', zero_division=0)\n",
    "f1_severity = f1_score(all_severity_labels, all_severity_preds, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'道路缺陷分类准确率: {accuracy_defect * 100:.2f}%')\n",
    "print(f'严重程度分类准确率: {accuracy_severity * 100:.2f}%')\n",
    "print(f'道路缺陷分类精确度: {precision_defect:.2f}')\n",
    "print(f'严重程度分类精确度: {precision_severity:.2f}')\n",
    "print(f'道路缺陷分类F1值: {f1_defect:.2f}')\n",
    "print(f'严重程度分类F1值: {f1_severity:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-09-10 21:10:22,722]\u001b[0m Finished trial#0 resulted in value: -0.65. Current best value is -0.65 with parameters: {'lr': 0.0001555203402272037, 'batch_size': 128, 'dropout_rate': 0.1933772703266984, 'num_filters1': 128, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:10:28,948]\u001b[0m Finished trial#1 resulted in value: -0.6950000000000001. Current best value is -0.6950000000000001 with parameters: {'lr': 1.2518519791871144e-05, 'batch_size': 32, 'dropout_rate': 0.28146522804254986, 'num_filters1': 128, 'num_filters2': 256}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:10:34,470]\u001b[0m Finished trial#2 resulted in value: -0.7050000000000001. Current best value is -0.7050000000000001 with parameters: {'lr': 1.359618913218973e-05, 'batch_size': 32, 'dropout_rate': 0.2943027572468289, 'num_filters1': 128, 'num_filters2': 128}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:10:41,810]\u001b[0m Finished trial#3 resulted in value: -0.7275. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:10:47,273]\u001b[0m Finished trial#4 resulted in value: -0.7025. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:10:52,398]\u001b[0m Finished trial#5 resulted in value: -0.6625. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:11:01,727]\u001b[0m Finished trial#6 resulted in value: -0.605. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:11:06,145]\u001b[0m Finished trial#7 resulted in value: -0.505. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:11:09,261]\u001b[0m Finished trial#8 resulted in value: -0.6074999999999999. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:11:19,709]\u001b[0m Finished trial#9 resulted in value: -0.6799999999999999. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:11:34,592]\u001b[0m Finished trial#10 resulted in value: -0.72. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:11:48,321]\u001b[0m Finished trial#11 resulted in value: -0.64. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:12:02,161]\u001b[0m Finished trial#12 resulted in value: -0.6174999999999999. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:12:15,831]\u001b[0m Finished trial#13 resulted in value: -0.6725000000000001. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:12:29,768]\u001b[0m Finished trial#14 resulted in value: -0.525. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:12:41,142]\u001b[0m Finished trial#15 resulted in value: -0.6525000000000001. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:12:55,493]\u001b[0m Finished trial#16 resulted in value: -0.7175. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:13:09,347]\u001b[0m Finished trial#17 resulted in value: -0.62. Current best value is -0.7275 with parameters: {'lr': 0.0036299288393169594, 'batch_size': 16, 'dropout_rate': 0.4673657097607763, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:13:17,240]\u001b[0m Finished trial#18 resulted in value: -0.73. Current best value is -0.73 with parameters: {'lr': 0.004770104165737106, 'batch_size': 16, 'dropout_rate': 0.45228785312285025, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:13:20,431]\u001b[0m Finished trial#19 resulted in value: -0.6825. Current best value is -0.73 with parameters: {'lr': 0.004770104165737106, 'batch_size': 16, 'dropout_rate': 0.45228785312285025, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:13:24,732]\u001b[0m Finished trial#20 resulted in value: -0.5349999999999999. Current best value is -0.73 with parameters: {'lr': 0.004770104165737106, 'batch_size': 16, 'dropout_rate': 0.45228785312285025, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:13:32,052]\u001b[0m Finished trial#21 resulted in value: -0.74. Current best value is -0.74 with parameters: {'lr': 0.009837922002676195, 'batch_size': 16, 'dropout_rate': 0.4996272660415808, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:13:40,026]\u001b[0m Finished trial#22 resulted in value: -0.5974999999999999. Current best value is -0.74 with parameters: {'lr': 0.009837922002676195, 'batch_size': 16, 'dropout_rate': 0.4996272660415808, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:13:50,562]\u001b[0m Finished trial#23 resulted in value: -0.565. Current best value is -0.74 with parameters: {'lr': 0.009837922002676195, 'batch_size': 16, 'dropout_rate': 0.4996272660415808, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:14:05,002]\u001b[0m Finished trial#24 resulted in value: -0.69. Current best value is -0.74 with parameters: {'lr': 0.009837922002676195, 'batch_size': 16, 'dropout_rate': 0.4996272660415808, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:14:19,702]\u001b[0m Finished trial#25 resulted in value: -0.665. Current best value is -0.74 with parameters: {'lr': 0.009837922002676195, 'batch_size': 16, 'dropout_rate': 0.4996272660415808, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:14:34,618]\u001b[0m Finished trial#26 resulted in value: -0.5825. Current best value is -0.74 with parameters: {'lr': 0.009837922002676195, 'batch_size': 16, 'dropout_rate': 0.4996272660415808, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:14:49,753]\u001b[0m Finished trial#27 resulted in value: -0.7. Current best value is -0.74 with parameters: {'lr': 0.009837922002676195, 'batch_size': 16, 'dropout_rate': 0.4996272660415808, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:15:04,246]\u001b[0m Finished trial#28 resulted in value: -0.745. Current best value is -0.745 with parameters: {'lr': 0.007916832081882586, 'batch_size': 16, 'dropout_rate': 0.4983031929840477, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n",
      "\u001b[32m[I 2024-09-10 21:15:12,135]\u001b[0m Finished trial#29 resulted in value: -0.5475. Current best value is -0.745 with parameters: {'lr': 0.007916832081882586, 'batch_size': 16, 'dropout_rate': 0.4983031929840477, 'num_filters1': 32, 'num_filters2': 64}.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳超参数:  {'lr': 0.007916832081882586, 'batch_size': 16, 'dropout_rate': 0.4983031929840477, 'num_filters1': 32, 'num_filters2': 64}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # 优化超参数\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    num_filters1 = trial.suggest_categorical('num_filters1', [32, 64, 128])\n",
    "    num_filters2 = trial.suggest_categorical('num_filters2', [64, 128, 256])\n",
    "  \n",
    "    # 划分训练集和验证集\n",
    "    X_train, X_val, y_train_defect, y_val_defect, y_train_severity, y_val_severity = train_test_split(\n",
    "        X_train_val, y_train_val_defect, y_train_val_severity, test_size=0.2, stratify=y_train_val_defect, random_state=42\n",
    "    )\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataset = AugmentedDataset(X_train, y_train_defect, y_train_severity, augment=True)\n",
    "    val_dataset = AugmentedDataset(X_val, y_val_defect, y_val_severity, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 定义模型\n",
    "    class CNN_Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN_Model, self).__init__()\n",
    "            self.conv1 = nn.Conv1d(in_channels=3, out_channels=num_filters1, kernel_size=5, stride=1, padding=2)\n",
    "            self.bn1 = nn.BatchNorm1d(num_filters1)\n",
    "            self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.conv2 = nn.Conv1d(in_channels=num_filters1, out_channels=num_filters2, kernel_size=3, stride=1, padding=1)\n",
    "            self.bn2 = nn.BatchNorm1d(num_filters2)\n",
    "            self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.fc1 = nn.Linear(num_filters2 * 12, 128)  # 调整全连接层大小\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.fc2 = nn.Linear(128, 64)  # 增加复杂度\n",
    "\n",
    "            self.fc_defect = nn.Linear(64, 3)  # 道路缺陷分类 (3 类)\n",
    "            self.fc_severity = nn.Linear(64, 4)  # 严重程度分类 (4 类)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "\n",
    "            x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = torch.relu(self.fc2(x))\n",
    "\n",
    "            defect_output = self.fc_defect(x)\n",
    "            severity_output = self.fc_severity(x)\n",
    "\n",
    "            return defect_output, severity_output\n",
    "\n",
    "    model = CNN_Model()\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 20  # 增加训练 epoch\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, defect_labels, severity_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            loss_defect = criterion(defect_preds, defect_labels)\n",
    "            loss_severity = criterion(severity_preds, severity_labels)\n",
    "            loss = loss_defect + loss_severity\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    all_defect_labels = []\n",
    "    all_defect_preds = []\n",
    "    all_severity_labels = []\n",
    "    all_severity_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, defect_labels, severity_labels in val_loader:\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            _, predicted_defects = torch.max(defect_preds, 1)\n",
    "            _, predicted_severities = torch.max(severity_preds, 1)\n",
    "\n",
    "            all_defect_labels.extend(defect_labels.cpu().numpy())\n",
    "            all_defect_preds.extend(predicted_defects.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "            all_severity_preds.extend(predicted_severities.cpu().numpy())\n",
    "\n",
    "    # 计算验证集上的准确率\n",
    "    accuracy_defect = accuracy_score(all_defect_labels, all_defect_preds)\n",
    "    accuracy_severity = accuracy_score(all_severity_labels, all_severity_preds)\n",
    "\n",
    "    avg_accuracy = (accuracy_defect + accuracy_severity) / 2\n",
    "    return -avg_accuracy\n",
    "\n",
    "# 调整搜索范围后，重新运行\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)  # 增加试验次数\n",
    "\n",
    "# 最佳超参数结果\n",
    "print('最佳超参数: ', study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集上的道路缺陷分类准确率: 68.50%\n",
      "验证集上的严重程度分类准确率: 65.50%\n"
     ]
    }
   ],
   "source": [
    "def train_and_test_with_best_params(best_params):\n",
    "    # 从最佳参数中提取超参数\n",
    "    learning_rate = best_params['lr']\n",
    "    batch_size = best_params['batch_size']\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    num_filters1 = best_params['num_filters1']\n",
    "    num_filters2 = best_params['num_filters2']\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    X_train, X_val, y_train_defect, y_val_defect, y_train_severity, y_val_severity = train_test_split(\n",
    "        X_train_val, y_train_val_defect, y_train_val_severity, test_size=0.2, stratify=y_train_val_defect, random_state=42\n",
    "    )\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataset = AugmentedDataset(X_train, y_train_defect, y_train_severity, augment=True)\n",
    "    val_dataset = AugmentedDataset(X_val, y_val_defect, y_val_severity, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 定义模型\n",
    "    class CNN_Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN_Model, self).__init__()\n",
    "            self.conv1 = nn.Conv1d(in_channels=3, out_channels=num_filters1, kernel_size=5, stride=1, padding=2)\n",
    "            self.bn1 = nn.BatchNorm1d(num_filters1)\n",
    "            self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.conv2 = nn.Conv1d(in_channels=num_filters1, out_channels=num_filters2, kernel_size=3, stride=1, padding=1)\n",
    "            self.bn2 = nn.BatchNorm1d(num_filters2)\n",
    "            self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.fc1 = nn.Linear(num_filters2 * 12, 128)  # 调整全连接层大小\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.fc2 = nn.Linear(128, 64)  # 增加复杂度\n",
    "\n",
    "            self.fc_defect = nn.Linear(64, 3)  # 道路缺陷分类 (3 类)\n",
    "            self.fc_severity = nn.Linear(64, 4)  # 严重程度分类 (4 类)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "\n",
    "            x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = torch.relu(self.fc2(x))\n",
    "\n",
    "            defect_output = self.fc_defect(x)\n",
    "            severity_output = self.fc_severity(x)\n",
    "\n",
    "            return defect_output, severity_output\n",
    "\n",
    "    model = CNN_Model()\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 20\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, defect_labels, severity_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            loss_defect = criterion(defect_preds, defect_labels)\n",
    "            loss_severity = criterion(severity_preds, severity_labels)\n",
    "            loss = loss_defect + loss_severity\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    all_defect_labels = []\n",
    "    all_defect_preds = []\n",
    "    all_severity_labels = []\n",
    "    all_severity_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, defect_labels, severity_labels in val_loader:\n",
    "            defect_preds, severity_preds = model(inputs)\n",
    "            _, predicted_defects = torch.max(defect_preds, 1)\n",
    "            _, predicted_severities = torch.max(severity_preds, 1)\n",
    "\n",
    "            all_defect_labels.extend(defect_labels.cpu().numpy())\n",
    "            all_defect_preds.extend(predicted_defects.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "            all_severity_preds.extend(predicted_severities.cpu().numpy())\n",
    "\n",
    "    # 计算验证集上的准确率\n",
    "    accuracy_defect = accuracy_score(all_defect_labels, all_defect_preds)\n",
    "    accuracy_severity = accuracy_score(all_severity_labels, all_severity_preds)\n",
    "\n",
    "    print(f\"验证集上的道路缺陷分类准确率: {accuracy_defect * 100:.2f}%\")\n",
    "    print(f\"验证集上的严重程度分类准确率: {accuracy_severity * 100:.2f}%\")\n",
    "\n",
    "# 使用最佳超参数重新训练和测试模型\n",
    "train_and_test_with_best_params(study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
